\section{Discussion Progress}
%%%***This is largely a recap of the results, but not much of a discussion about why.  A discussion should careful consider why you got the results you did, and how it relates to what you expected and other works.

The results indicate that spectral features are more powerful features for all machine learning algorithms. The reason behind this might be the number of features and their qualities in this set. Based on table \ref{tab:Features_list}, there are about 300 features for this set, much more than for the other two.

Another feature that has a significant effect on the accuracy is inter-stride distance. This feature has been added only to the all feature set, and it is one of the powerful features. Furthermore, other inter-stride features like the angle used to align the footprint with the template and stride duration should be considered in the feature set to improve the discrimination power of classifiers.  


%The results might suggest that using AR features are not significant impact on accuracy. The accuracy of LDA without these features reduced to 68.57\%. 

Unbalance dataset also could have a negative effect on the results. For example in the Stepscan dataset, there is about 800 negative class whereas 16 samples associated with the positive class. Consequently, the models were biased towards the negative class. 

The poor results of the transfer learning framework might be because only machine learning algorithms were trained based on the Stepscan dataset. It could have a better performance if we train the last two-layer of CNNs. 





%The LDA classifier’s best performance was a 69.2\% accuracy, as shown in table \ref{tab:1_ML}. This result comes from classifying the combination of all features. The kNN algorithm had similar performance on all kinds of features except \gls{AR} features. All algorithms had the worst results on this group of features.

%The SVM performed best with the mixture of all features, at 56\% accuracy (see table \ref{tab:1_ML}). All other feature subsets achieved lower than this amount. The worst performance for all these classifiers belonged to the random forest classifier with about 30\%.

%The results do not fit with the theory that…
%The experiment provides a new insight into the relationship between…

%The data contributes a clearer understanding of…
%While previous research has focused on X, these results demonstrate that Y.
%The generalizability of the results is limited by…
%The reliability of this data is impacted by…
%The methodological choices were constrained by…
%It is beyond the scope of this study to…




