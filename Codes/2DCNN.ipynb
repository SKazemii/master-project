{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from MLPackage.Deep_network import * \n",
    "\n",
    "class Pipeline(Classifier, Seamese):\n",
    "    \n",
    "    _col = [\n",
    "        \"test_id\",\n",
    "        \"subject\",\n",
    "        \"combination\",\n",
    "        \"classifier_name\",\n",
    "        \"normilizing\",\n",
    "        \"persentage\",\n",
    "        \"EER\",\n",
    "        \"TH\",\n",
    "        \"ACC_bd\",\n",
    "        \"BACC_bd\",\n",
    "        \"FAR_bd\",\n",
    "        \"FRR_bd\",\n",
    "        \"ACC_ud\",\n",
    "        \"BACC_ud\",\n",
    "        \"FAR_ud\",\n",
    "        \"FRR_ud\",\n",
    "        \"AUS\",\n",
    "        \"FAU\",\n",
    "        \"unknown_imposter_samples\",\n",
    "        \"AUS_All\",\n",
    "        \"FAU_All\",\n",
    "        \"CM_bd_TN\",\n",
    "        \"CM_bd_FP\",\n",
    "        \"CM_bd_FN\",\n",
    "        \"CM_bd_TP\",\n",
    "        \"CM_ud_TN\",\n",
    "        \"CM_ud_FP\",\n",
    "        \"CM_ud_FN\",\n",
    "        \"CM_ud_TP\",\n",
    "        \"num_pc\",\n",
    "        \"KFold\",\n",
    "        \"p_training_samples\",\n",
    "        \"train_ratio\",\n",
    "        \"ratio\",\n",
    "        # pos_te_samples,\n",
    "        # neg_te_samples,\n",
    "        \"known_imposter\",\n",
    "        \"unknown_imposter\",\n",
    "        \"min_number_of_sample\",\n",
    "        \"number_of_unknown_imposter_samples\",\n",
    "        \"y_train.shape[0]\",\n",
    "        \"y_train.sum()\",\n",
    "        \"y_val.shape[0]\",\n",
    "        \"y_val.sum()\",\n",
    "        \"y_test.shape[0]\",\n",
    "        \"y_test.sum()\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, kwargs):\n",
    "\n",
    "        self.dataset_name = \"\"\n",
    "        self._combination = 0\n",
    "\n",
    "        self._labels = 0\n",
    "\n",
    "        self._GRFs = pd.DataFrame()\n",
    "        self._COAs = pd.DataFrame()\n",
    "        self._COPs = pd.DataFrame()\n",
    "        self._pre_images = pd.DataFrame()\n",
    "\n",
    "        self._COA_handcrafted = pd.DataFrame()\n",
    "        self._COP_handcrafted = pd.DataFrame()\n",
    "        self._GRF_handcrafted = pd.DataFrame()\n",
    "\n",
    "        self._GRF_WPT = pd.DataFrame()\n",
    "        self._COP_WPT = pd.DataFrame()\n",
    "        self._COA_WPT = pd.DataFrame()\n",
    "\n",
    "        self._deep_features = pd.DataFrame()\n",
    "\n",
    "        self._CNN_base_model = \"\"\n",
    "\n",
    "        self._CNN_weights = \"imagenet\"\n",
    "        self._CNN_include_top = False\n",
    "        self._verbose = False\n",
    "        self._CNN_batch_size = 32\n",
    "        self._CNN_epochs = 10\n",
    "        self._CNN_optimizer = \"adam\"\n",
    "        self._val_size = 0.2\n",
    "\n",
    "        #####################################################\n",
    "        self._CNN_class_numbers = 97\n",
    "        self._CNN_epochs = 10\n",
    "        self._CNN_image_size = (60, 40, 3)\n",
    "\n",
    "        self._min_number_of_sample = 30\n",
    "        self._known_imposter = 5\n",
    "        self._unknown_imposter = 30\n",
    "        self._number_of_unknown_imposter_samples = 1.0  # Must be less than 1\n",
    "\n",
    "        # self._known_imposter_list   = []\n",
    "        # self._unknown_imposter_list = []\n",
    "\n",
    "        self._waveletname = \"coif1\"\n",
    "        self._pywt_mode = \"constant\"\n",
    "        self._wavelet_level = 4\n",
    "\n",
    "        self._KFold = 10\n",
    "        self._random_state = 42\n",
    "\n",
    "        self._p_training_samples = 11\n",
    "        self._train_ratio = 4\n",
    "        self._ratio = True\n",
    "\n",
    "        self._classifier_name = \"\"\n",
    "\n",
    "        self._KNN_n_neighbors = 5\n",
    "        self._KNN_metric = \"euclidean\"\n",
    "        self._KNN_weights = \"uniform\"\n",
    "        self._SVM_kernel = \"linear\"\n",
    "        self._random_runs = 10\n",
    "        self._THRESHOLDs = np.linspace(0, 1, 100)\n",
    "        self._persentage = 0.95\n",
    "        self._normilizing = \"z-score\"\n",
    "\n",
    "        self._num_pc = 0\n",
    "\n",
    "        for (key, value) in kwargs.items():\n",
    "            if key in self.__dict__:\n",
    "                setattr(self, key, value)\n",
    "            else:\n",
    "                logger.error(\"key must be one of these:\", self.__dict__.keys())\n",
    "                raise KeyError(key)\n",
    "\n",
    "        super().__init__(self.dataset_name, self._classifier_name)\n",
    "\n",
    "    def run(self, DF_features_all: pd.DataFrame, feature_set_names: list):\n",
    "\n",
    "        DF_known_imposter, DF_unknown_imposter = self.filtering_subjects_and_samples(\n",
    "            DF_features_all\n",
    "        )\n",
    "        DF_unknown_imposter = DF_unknown_imposter.dropna()\n",
    "        DF_known_imposter = DF_known_imposter.dropna()\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # # extract features of shod dataset to use as unknown imposter samples\n",
    "        # # it is overwrite on DF_unknown_imposter DataFrame\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # DF_features_all_shod, feature_set_names_shod = self.extracting_feature_set1('casia-shod')\n",
    "        # DF_unknown_imposter = DF_features_all_shod[DF_features_all_shod['side']>=2.0].dropna()\n",
    "        # subjects, samples = np.unique(DF_unknown_imposter[\"ID\"].values, return_counts=True)\n",
    "\n",
    "        # self._unknown_imposter_list = subjects[-self._unknown_imposter:]\n",
    "        # DF_unknown_imposter =  DF_unknown_imposter[DF_unknown_imposter[\"ID\"].isin(self._unknown_imposter_list)]\n",
    "\n",
    "        # self.set_dataset_path('casia')\n",
    "        # breakpoint()\n",
    "        # ----------------------------------------------------------------\n",
    "\n",
    "        results = list()\n",
    "        for idx, subject in enumerate(self._known_imposter_list):\n",
    "            # if idx not in [0, 1]: #todo: remove this block to run for all subjects.\n",
    "            #     break\n",
    "\n",
    "            if self._verbose == True:\n",
    "                logger.info(\n",
    "                    f\"   Subject number: {idx} out of {len(self._known_imposter_list)} (subject ID is {subject})\"\n",
    "                )\n",
    "\n",
    "            # #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            # # # droping shod samples from known imposter in training set\n",
    "            # # # it is overwrite on DF_unknown_imposter DataFrame\n",
    "            # #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            # index_of_shod_samples = DF_known_imposter[ (DF_known_imposter['side'] >= 2) & (DF_known_imposter['ID'] == subject)].index\n",
    "            # DF_known_imposter1 = DF_known_imposter.drop(index_of_shod_samples)\n",
    "            # #----------------------------------------------------------------\n",
    "\n",
    "            # #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            # # # droping barefoot samples from unknown imposter\n",
    "            # # # it is overwrite on DF_unknown_imposter DataFrame\n",
    "            # #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            # index_of_barefoot_samples = DF_unknown_imposter[ DF_unknown_imposter['side'] <= 1 ].index\n",
    "            # DF_unknown_imposter = DF_unknown_imposter.drop(index_of_barefoot_samples)\n",
    "            # #----------------------------------------------------------------\n",
    "\n",
    "            (\n",
    "                DF_known_imposter_binariezed,\n",
    "                DF_unknown_imposter_binariezed,\n",
    "            ) = self.binarize_labels(DF_known_imposter, DF_unknown_imposter, subject)\n",
    "\n",
    "            # #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            # # # applying template selection on known imposters\n",
    "            # # # it is select only 200 samples from all knowwn imposters\n",
    "            # #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            # A1 = DF_known_imposter_binariezed[DF_known_imposter_binariezed['ID'] == 1.0]\n",
    "            # A2 = DF_known_imposter_binariezed[DF_known_imposter_binariezed['ID'] == 0.0]\n",
    "            # A2 = self.template_selection(A2, 'DEND', 200, verbose=True)\n",
    "            # DF_known_imposter_binariezed = pd.concat([A1, A2], axis=0)\n",
    "            # # breakpoint()\n",
    "            # #----------------------------------------------------------------\n",
    "\n",
    "            CV = model_selection.StratifiedKFold(\n",
    "                n_splits=self._KFold, shuffle=False\n",
    "            )  # random_state=self._random_state,\n",
    "            X = DF_known_imposter_binariezed\n",
    "            U = DF_unknown_imposter_binariezed\n",
    "\n",
    "            cv_results = list()\n",
    "\n",
    "            ncpus = int(\n",
    "                os.environ.get(\n",
    "                    \"SLURM_CPUS_PER_TASK\", default=multiprocessing.cpu_count()\n",
    "                )\n",
    "            )\n",
    "            pool = multiprocessing.Pool(processes=ncpus)\n",
    "\n",
    "            for fold, (train_index, test_index) in enumerate(\n",
    "                CV.split(X.iloc[:, :-1], X.iloc[:, -1])\n",
    "            ):\n",
    "                # breakpoint()\n",
    "                # res = pool.apply_async(self.fold_calculating, args=(feature_set_names, subject, X, U, train_index, test_index, fold,))#, callback=print)#cv_results.append)\n",
    "                # print(res.get())  # this will raise an exception if it happens within func\n",
    "\n",
    "                cv_results.append(\n",
    "                    self.fold_calculating(\n",
    "                        feature_set_names, subject, X, U, train_index, test_index, fold\n",
    "                    )\n",
    "                )  # todo: comment this line to run all folds\n",
    "                # break #todo: comment this line to run all folds\n",
    "\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            # breakpoint()\n",
    "            result = self.compacting_results(cv_results, subject)\n",
    "            results.append(result)\n",
    "\n",
    "        return pd.DataFrame(results, columns=self._col)\n",
    "\n",
    "    def compacting_results(self, results, subject):\n",
    "        # [EER, TH, ACC_bd, BACC_bd, FAR_bd, FRR_bd, ACC_ud, BACC_ud, FAR_ud, FRR_ud,]\n",
    "\n",
    "        # return results, CM_bd, CM_ud\n",
    "        # breakpoint()\n",
    "        # pos_te_samples = self._p\n",
    "        # neg_te_samples = self._\n",
    "        # pos_tr_samples = self._\n",
    "        # neg_tr_ratio = self._\n",
    "\n",
    "        result = list()\n",
    "\n",
    "        result.append(\n",
    "            [\n",
    "                self._test_id,\n",
    "                subject,\n",
    "                self._combination,\n",
    "                self._classifier_name,\n",
    "                self._normilizing,\n",
    "                self._persentage,\n",
    "                # configs[\"classifier\"][CLS],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        result.append(np.array(results).mean(axis=0))\n",
    "        # result.append([np.array(CM_bd).mean(axis=0), np.array(CM_ud).mean(axis=0)])\n",
    "\n",
    "        # _CNN_weights = 'imagenet'\n",
    "        # _CNN_base_model = \"\"\n",
    "\n",
    "        result.append(\n",
    "            [\n",
    "                self._KFold,\n",
    "                self._p_training_samples,\n",
    "                self._train_ratio,\n",
    "                self._ratio,\n",
    "                # pos_te_samples,\n",
    "                # neg_te_samples,\n",
    "                self._known_imposter,\n",
    "                self._unknown_imposter,\n",
    "                self._min_number_of_sample,\n",
    "                self._number_of_unknown_imposter_samples,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return [val for sublist in result for val in sublist]\n",
    "\n",
    "    def fold_calculating(self, feature_set_names: list, subject: int, x_train, x_test, U, train_index, val_index, fold):\n",
    "\n",
    "        logger.info(f\"\\tFold number: {fold} out of {self._KFold} ({os.getpid()})\")\n",
    "        df_train = x_train.iloc[train_index, :]\n",
    "        df_val = x_train.iloc[val_index, :]\n",
    "        # breakpoint()\n",
    "\n",
    "        # df_train = self.down_sampling_new(df_train, 2)\n",
    "        \n",
    "        df_train, df_val, df_test, df_test_U = self.scaler(df_train, df_val, x_test, U)\n",
    "\n",
    "        df_train, df_val, df_test, df_test_U, num_pc = self.projector(feature_set_names, df_train, df_val, df_test, df_test_U, )\n",
    "        results = self.ML_classifier(subject, x_train=df_train, x_val=df_val, x_test=df_test, x_test_U=df_test_U)\n",
    "\n",
    "        results[\"num_pc\"] = num_pc\n",
    "        results.update({\n",
    "\n",
    "            \"training_samples\": df_train.shape[0],\n",
    "            \"pos_training_samples\": df_train['ID'].sum(),\n",
    "            \"validation_samples\": 0,\n",
    "            \"pos_validation_samples\": 0,\n",
    "            \"testing_samples\": df_val.shape[0],\n",
    "            \"pos_testing_samples\": df_val['ID'].sum(),\n",
    "        })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def collect_results(self, result: pd.DataFrame, pipeline_name: str) -> None:\n",
    "        # result['pipeline'] = pipeline_name\n",
    "        test = os.environ.get(\"SLURM_JOB_NAME\", default=pipeline_name)\n",
    "        excel_path = os.path.join(os.getcwd(), \"results\", f\"Result__{test}.xlsx\")\n",
    "\n",
    "        if os.path.isfile(excel_path):\n",
    "            Results_DF = pd.read_excel(excel_path, index_col=0)\n",
    "        else:\n",
    "            Results_DF = pd.DataFrame(columns=self._col)\n",
    "\n",
    "        Results_DF = Results_DF.append(result)\n",
    "        try:\n",
    "            Results_DF.to_excel(excel_path)\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            Results_DF.to_excel(excel_path[:-5] + str(self._test_id) + \".xlsx\")\n",
    "\n",
    "    def extracting_feature_set1(self, dataset_name: str) -> pd.DataFrame:\n",
    "        GRFs, COPs, COAs, pre_images, labels = self.loading_pre_features(dataset_name)\n",
    "        COA_handcrafted = self.loading_COA_handcrafted(COAs)\n",
    "        COP_handcrafted = self.loading_COP_handcrafted(COPs)\n",
    "        GRF_handcrafted = self.loading_GRF_handcrafted(GRFs)\n",
    "        COA_WPT = self.loading_COA_WPT(COAs)\n",
    "        COP_WPT = self.loading_COP_WPT(COPs)\n",
    "        GRF_WPT = self.loading_GRF_WPT(GRFs)\n",
    "\n",
    "        # deep_features_list = A.loading_deep_features_from_list((pre_images, labels), ['P100', 'P80'], 'resnet50.ResNet50')\n",
    "        # image_from_list = A.loading_pre_image_from_list(pre_images, ['P80', 'P100'])\n",
    "        # P70 = A.loading_pre_image(pre_images, 'P70')\n",
    "        # P90 = A.loading_deep_features((pre_images, labels), 'P90', 'resnet50.ResNet50')\n",
    "\n",
    "        feature_set_names = [\n",
    "            \"COP_handcrafted\",\n",
    "            \"COPs\",\n",
    "            \"COP_WPT\",\n",
    "            \"GRF_handcrafted\",\n",
    "            \"GRFs\",\n",
    "            \"GRF_WPT\",\n",
    "        ]\n",
    "        feature_set = []\n",
    "        for i in feature_set_names:\n",
    "            feature_set.append(eval(f\"{i}\"))\n",
    "\n",
    "        return pd.concat(feature_set + [labels], axis=1), feature_set_names\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimizer_accross_subjects(Users, no_samples, classifier):\n",
    "    setting = {\n",
    "        \"dataset_name\": \"casia\",\n",
    "        \"_classifier_name\": \"knn\",\n",
    "        \"_combination\": True,\n",
    "        \"_CNN_weights\": \"imagenet\",\n",
    "        \"_verbose\": True,\n",
    "        \"_CNN_batch_size\": 32,\n",
    "        \"_CNN_base_model\": \"\",\n",
    "        \"_CNN_epochs\": 500,\n",
    "        \"_CNN_optimizer\": \"adam\",\n",
    "        \"_val_size\": 0.2,\n",
    "        \"_min_number_of_sample\": 30,\n",
    "        \"_known_imposter\": 32,\n",
    "        \"_unknown_imposter\": 32,\n",
    "        \"_number_of_unknown_imposter_samples\": 1.0,  # Must be less than 1\n",
    "        \"_waveletname\": \"coif1\",\n",
    "        \"_pywt_mode\": \"constant\",\n",
    "        \"_wavelet_level\": 4,\n",
    "        \"_p_training_samples\": 11,\n",
    "        \"_train_ratio\": 34,\n",
    "        \"_ratio\": False,\n",
    "        \"_KNN_n_neighbors\": 5,\n",
    "        \"_KNN_metric\": \"euclidean\",\n",
    "        \"_KNN_weights\": \"uniform\",\n",
    "        \"_SVM_kernel\": \"linear\",\n",
    "        \"_KFold\": 10,\n",
    "        \"_random_runs\": 20,\n",
    "        \"_persentage\": 0.95,\n",
    "        \"_normilizing\": \"z-score\",\n",
    "    }\n",
    "\n",
    "    A = Pipeline(setting)\n",
    "\n",
    "    A._known_imposter = Users\n",
    "    A._unknown_imposter = 10\n",
    "    A._classifier_name = classifier\n",
    "\n",
    "    image_feature_name = [\"P80\", \"P100\" ]  \n",
    "    dataset_name = \"casia\"\n",
    "\n",
    "    GRFs, COPs, COAs, pre_images, labels = A.loading_pre_features(dataset_name)\n",
    "\n",
    "   \n",
    "\n",
    "    ####################################################################################################################\n",
    "    # pipeline 1: P100 and P80\n",
    "    image_from_list = A.loading_pre_image_from_list(pre_images, image_feature_name)\n",
    "    feature_set_names = [\"P80\", \"P100\"]\n",
    "    DF_feature_all = pd.concat([i for i in image_from_list] + [labels], axis=1)\n",
    "    \n",
    "    subjects, samples = np.unique(DF_feature_all[\"ID\"].values, return_counts=True)\n",
    "\n",
    "    ss = [a[0] for a in list(zip(subjects, samples)) if a[1] >= A._min_number_of_sample]\n",
    "\n",
    "    known_imposter_list = ss[:A._known_imposter]\n",
    "    unknown_imposter_list = ss[-A._unknown_imposter :]\n",
    "\n",
    "    DF_unknown_imposter = DF_feature_all[ DF_feature_all[\"ID\"].isin(unknown_imposter_list) ]\n",
    "    DF_known_imposter = DF_feature_all[DF_feature_all[\"ID\"].isin(known_imposter_list)]\n",
    "\n",
    "    search = {\n",
    "            'knn': {'n_neighbors': [1, 20]},\n",
    "            'svm-linear': {'logC': [-4, 3]},\n",
    "            'svm-rbf': {'logGamma': [-6, 0], 'logC': [-4, 3]},\n",
    "            'svm-poly': {'logGamma': [2, 5], 'logC': [-4, 3], 'coef0': [0, 1]},\n",
    "            'rf': {'n_estimators': [20, 120], 'max_features': [5, 25]},\n",
    "            'if': {'n_estimators': [20, 120], 'max_features': [5, 25]},\n",
    "            'ocsvm': { 'nu': [0, 1]},\n",
    "            'svdd': {'nu': [0, 1], 'logGamma': [-6, 0]},\n",
    "            'tm': None,\n",
    "            'lda': None,\n",
    "        }\n",
    "        \n",
    "    \n",
    "\n",
    "    def performance(DF_known_imposter, DF_unknown_imposter,):\n",
    "        def objective_func(\n",
    "                    n_neighbors=None, metric=None, weights=None,\n",
    "                    logC=None, logGamma=None, degree=None, coef0=None,\n",
    "                    n_estimators=None, max_features=None,\n",
    "                    nu=None):\n",
    "\n",
    "            lst = []\n",
    "\n",
    "            for idx, subject in enumerate(DF_known_imposter[\"ID\"].unique()):\n",
    "\n",
    "                non_targets = DF_known_imposter[DF_known_imposter[\"ID\"]!=subject]\n",
    "                non_targets = non_targets.groupby(\"ID\", group_keys=False).apply(lambda x: x.sample( n=no_samples, replace=False, random_state=A._random_state))\n",
    "                target = DF_known_imposter[DF_known_imposter[\"ID\"]==subject]\n",
    "                DF = pd.concat([target, non_targets], axis=0)\n",
    "\n",
    "                X, U = A.binarize_labels(DF, DF_unknown_imposter, subject)\n",
    "                \n",
    "                x_train, x_test = model_selection.train_test_split(X, test_size=0.20, random_state=A._random_state, stratify=X.iloc[:, -1].values,)\n",
    "                x_train, x_val = model_selection.train_test_split(x_train, test_size=0.20, random_state=A._random_state, stratify=x_train.iloc[:, -1].values)\n",
    "\n",
    "                df_train, df_val, df_test, df_test_U = A.scaler(x_train, x_val, x_test, U)\n",
    "                df_train, df_val, df_test, df_test_U, num_pc = A.projector(feature_set_names, df_train, df_val, df_test, df_test_U, )\n",
    "\n",
    "                \n",
    "                # param, info = A.subject_optimizer(df_train, 2, 30, search[A._classifier_name])\n",
    "                \n",
    "                if   A._classifier_name == 'knn':\n",
    "                    if int(n_neighbors) < 1:\n",
    "                        return 0\n",
    "                    else:\n",
    "                        classifier = knn( n_neighbors=int(n_neighbors))#, metric=metric, weights=weights, )\n",
    "                        best_model = classifier.fit(df_train.iloc[:, :-1].values, df_train.iloc[:, -1].values)\n",
    "                        y_pred_tr = best_model.predict_proba(df_train.iloc[:, :-1].values)[:, 1]\n",
    "\n",
    "                        FRR_t, FAR_t = A.FXR_calculater(df_train.iloc[:, -1].values, y_pred_tr)\n",
    "                        EER, t_idx = A.compute_eer(FRR_t, FAR_t)\n",
    "                        TH = A._THRESHOLDs[t_idx]\n",
    "\n",
    "                        y_pred = best_model.predict_proba(df_val.iloc[:, :-1].values)[:, 1]\n",
    "                        \n",
    "                        y_pred[y_pred >= TH] = 1\n",
    "                        y_pred[y_pred < TH] = 0\n",
    "                elif A._classifier_name == 'svm-linear':\n",
    "                    classifier = svm.SVC(kernel='linear', probability=True, random_state=A._random_state, C=10 ** logC)\n",
    "                    best_model = classifier.fit(df_train.iloc[:, :-1].values, df_train.iloc[:, -1].values)\n",
    "                    y_pred_tr = best_model.predict_proba(df_train.iloc[:, :-1].values)[:, 1]\n",
    "\n",
    "                    FRR_t, FAR_t = A.FXR_calculater(df_train.iloc[:, -1].values, y_pred_tr)\n",
    "                    EER, t_idx = A.compute_eer(FRR_t, FAR_t)\n",
    "                    TH = A._THRESHOLDs[t_idx]\n",
    "\n",
    "                    y_pred = best_model.predict_proba(df_val.iloc[:, :-1].values)[:, 1]\n",
    "                    \n",
    "                    y_pred[y_pred >= TH] = 1\n",
    "                    y_pred[y_pred < TH] = 0\n",
    "                elif A._classifier_name == 'svm-poly':\n",
    "                    classifier = svm.SVC(kernel='poly', probability=True, random_state=A._random_state , C=10 ** logC, degree=degree, coef0=coef0)\n",
    "                    best_model = classifier.fit(df_train.iloc[:, :-1].values, df_train.iloc[:, -1].values)\n",
    "                    y_pred_tr = best_model.predict_proba(df_train.iloc[:, :-1].values)[:, 1]\n",
    "\n",
    "                    FRR_t, FAR_t = A.FXR_calculater(df_train.iloc[:, -1].values, y_pred_tr)\n",
    "                    EER, t_idx = A.compute_eer(FRR_t, FAR_t)\n",
    "                    TH = A._THRESHOLDs[t_idx]\n",
    "\n",
    "                    y_pred = best_model.predict_proba(df_val.iloc[:, :-1].values)[:, 1]\n",
    "                    \n",
    "                    y_pred[y_pred >= TH] = 1\n",
    "                    y_pred[y_pred < TH] = 0\n",
    "                elif A._classifier_name == 'svm-rbf':\n",
    "                    classifier = svm.SVC(kernel='rbf', probability=True, random_state=A._random_state, C=10 ** logC, gamma=10 ** logGamma)\n",
    "                    best_model = classifier.fit(df_train.iloc[:, :-1].values, df_train.iloc[:, -1].values)\n",
    "                    y_pred_tr = best_model.predict_proba(df_train.iloc[:, :-1].values)[:, 1]\n",
    "\n",
    "                    FRR_t, FAR_t = A.FXR_calculater(df_train.iloc[:, -1].values, y_pred_tr)\n",
    "                    EER, t_idx = A.compute_eer(FRR_t, FAR_t)\n",
    "                    TH = A._THRESHOLDs[t_idx]\n",
    "\n",
    "                    y_pred = best_model.predict_proba(df_val.iloc[:, :-1].values)[:, 1]\n",
    "                    \n",
    "                    y_pred[y_pred >= TH] = 1\n",
    "                    y_pred[y_pred < TH] = 0\n",
    "                elif A._classifier_name == \"rf\":\n",
    "                    classifier = RandomForestClassifier(n_estimators=int(n_estimators), max_features=int(max_features))\n",
    "                    best_model = classifier.fit(df_train.iloc[:, :-1].values, df_train.iloc[:, -1].values)\n",
    "                    y_pred_tr = best_model.predict_proba(df_train.iloc[:, :-1].values)[:, 1]\n",
    "\n",
    "                    FRR_t, FAR_t = A.FXR_calculater(df_train.iloc[:, -1].values, y_pred_tr)\n",
    "                    EER, t_idx = A.compute_eer(FRR_t, FAR_t)\n",
    "                    TH = A._THRESHOLDs[t_idx]\n",
    "\n",
    "                    y_pred = best_model.predict_proba(df_val.iloc[:, :-1].values)[:, 1]\n",
    "                    \n",
    "                    y_pred[y_pred >= TH] = 1\n",
    "                    y_pred[y_pred < TH] = 0\n",
    "                elif A._classifier_name == \"nb\":\n",
    "                    pass\n",
    "                elif A._classifier_name == \"if\":\n",
    "                    classifier = IsolationForest(n_estimators=int(n_estimators), max_features=int(max_features), random_state=A._random_state)\n",
    "                    best_model = classifier.fit(df_train.iloc[:, :-1].values)\n",
    "                    EER = 0\n",
    "                    TH = 0\n",
    "                    y_pred = best_model.predict(df_val.iloc[:, :-1].values)\n",
    "                elif A._classifier_name == \"ocsvm\":\n",
    "                    if (nu <= 0) or (nu > 1):\n",
    "                        return 0\n",
    "                    else:\n",
    "                        classifier = OneClassSVM(kernel='linear', nu=nu)\n",
    "                        best_model = classifier.fit(df_train.iloc[:, :-1].values)\n",
    "                        EER = 0\n",
    "                        TH = 0\n",
    "\n",
    "                        y_pred = best_model.predict(df_val.iloc[:, :-1].values)\n",
    "                        y_pred = 0.5-(y_pred/2)\n",
    "                elif A._classifier_name == \"svdd\":\n",
    "                    if (nu <= 0) or (nu > 1):\n",
    "                        return 0\n",
    "                    else:\n",
    "                        classifier = OneClassSVM(kernel='rbf', nu=nu, gamma=10 ** logGamma)\n",
    "                        best_model = classifier.fit(df_train.iloc[:, :-1].values)\n",
    "                        EER = 0\n",
    "                        TH = 0\n",
    "\n",
    "                        y_pred = best_model.predict(df_val.iloc[:, :-1].values)\n",
    "                        y_pred = 0.5-(y_pred/2)\n",
    "                elif A._classifier_name == \"lda\":\n",
    "                    pass\n",
    "                elif A._classifier_name == \"tm\":\n",
    "                    pass\n",
    "                else:\n",
    "                        raise(f'Unknown algorithm: {A._classifier_name}')\n",
    "\n",
    "                lst.append(optunity.metrics.bacc(df_val.iloc[:, -1].values, y_pred, 1))\n",
    "            \n",
    "            \n",
    "            logger.info(f\"mean bacc: {np.mean(lst)}\")\n",
    "            return np.mean(lst)\n",
    "        \n",
    "        \n",
    "        return objective_func\n",
    "\n",
    "    objective_func = performance(DF_known_imposter, DF_unknown_imposter,)\n",
    "    \n",
    "    param = None\n",
    "    pmap8 = optunity.parallel.create_pmap(32)\n",
    "    if A._classifier_name in ['svdd', 'ocsvm', \"knn\", \"svm-rbf\", \"svm-linear\"]:\n",
    "        solver = optunity.solvers.ParticleSwarm(num_particles=32, num_generations=10, **search[A._classifier_name])\n",
    "        param, info = optunity.optimize(solver, objective_func, pmap=pmap8, maximize=True) # , pmap=pmap8\n",
    "        print(optunity.call_log2dataframe(info.call_log))\n",
    "\n",
    "    for idx, subject in enumerate(DF_known_imposter[\"ID\"].unique()):\n",
    "\n",
    "        logger.info(f\"   Subject number: {idx} out of {len(known_imposter_list)} (subject ID is {subject})\")\n",
    "\n",
    "\n",
    "        X, U = A.binarize_labels(DF_known_imposter, DF_unknown_imposter, subject)\n",
    "        x_train, x_test = model_selection.train_test_split(X, test_size=0.20, random_state=A._random_state, stratify=X.iloc[:, -1].values,)\n",
    "        _, x_val = model_selection.train_test_split(x_train, test_size=0.20, random_state=A._random_state, stratify=x_train.iloc[:, -1].values)\n",
    "\n",
    "        df_train, df_test, df_test_U = A.scaler(x_train, x_test, U)\n",
    "        df_train, df_test, df_test_U, num_pc = A.projector(feature_set_names, df_train, df_test, df_test_U, )\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # df_train = self.down_sampling_new(df_train, 2)\n",
    "        results = A.ML_classifier(subject, x_train=df_train, x_test=df_test, x_test_U=df_test_U, params=param)\n",
    "\n",
    "        results[\"num_pc\"] = num_pc\n",
    "        results.update({\n",
    "            \"training_samples\": df_train.shape[0],\n",
    "            \"pos_training_samples\": df_train['ID'].sum(),\n",
    "            \"validation_samples\": x_val.shape[0],\n",
    "            \"pos_validation_samples\": x_val['ID'].sum(),\n",
    "            \"testing_samples\": df_test.shape[0],\n",
    "            \"pos_testing_samples\": df_test['ID'].sum(),\n",
    "        })\n",
    "        \n",
    " \n",
    "        results.update( {\n",
    "            \"test_id\": A._test_id,\n",
    "            \"subject\": subject,\n",
    "            \"combination\": A._combination,\n",
    "            \"classifier_name\": A._classifier_name,\n",
    "            \"normilizing\": A._normilizing,\n",
    "            \"persentage\": A._persentage,\n",
    "            \"KFold\": \"-\",\n",
    "            \"known_imposter\": A._known_imposter,\n",
    "            \"unknown_imposter\": A._unknown_imposter,\n",
    "            \"min_number_of_sample\": A._min_number_of_sample,\n",
    "            \"param\": param,\n",
    "          \n",
    "        })\n",
    "\n",
    "        for i in results:\n",
    "            try:\n",
    "                res_dict[i].append(results[i])\n",
    "            except UnboundLocalError:\n",
    "                res_dict = {i: [] for i in results.keys()}\n",
    "                res_dict[i].append(results[i])\n",
    "    \n",
    "    return pd.DataFrame.from_dict(res_dict)\n",
    "   \n",
    "print(\"Done\")\n",
    "# results1 = optimizer_accross_subjects(2, 30, \"lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = optimizer_accross_subjects(2, 20, \"knn\")\n",
    "path = os.path.join(os.getcwd(), \"results\", \"accross_subj_6_20.xlsx\")\n",
    "results1.to_excel(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "8b82dd2663990a16b1957724494e3f14e462c9d7875192c2dd1af4201a2ca72e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
